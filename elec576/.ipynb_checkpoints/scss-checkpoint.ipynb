{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Channel Source Separation\n",
    "Some ideas:\n",
    "- Deeper networks\n",
    "- Wider networks\n",
    "- Different conv filter sizes\n",
    "- Noisy inputs\n",
    "- Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, errno\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_directory(f):\n",
    "    \"\"\"Makes directory if does not already exist\"\"\"\n",
    "    try:\n",
    "        os.makedirs(f)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _check_list(arg):\n",
    "    if isinstance(arg, list):\n",
    "        try:\n",
    "            return arg[0], arg[1:]\n",
    "        except IndexError:\n",
    "            return arg[0], []\n",
    "    else:\n",
    "        return arg, []\n",
    "\n",
    "def _get_variable_initializer(init_type, var_shape, *args):\n",
    "    if init_type == \"random_normal\":\n",
    "        mean = float(args[0])\n",
    "        stddev = float(args[1])\n",
    "        return tf.random_normal(var_shape, mean=mean, stddev=stddev)\n",
    "    elif init_type == \"truncated_normal\":\n",
    "        mean = float(args[0])\n",
    "        stddev = float(args[1])\n",
    "        return tf.truncated_normal(var_shape, mean=mean, stddev=stddev)\n",
    "    elif init_type == \"constant\":\n",
    "        c = args[0]\n",
    "        return tf.constant(c, dtype=tf.float32, shape=var_shape)\n",
    "    elif init_type == \"xavier\":\n",
    "        n_in = tf.cast(args[0], tf.float32)\n",
    "        return tf.div(tf.random_normal(var_shape), tf.sqrt(n_in))\n",
    "    else:\n",
    "        raise ValueError(\"Variable initializer \\\"\" + init_type + \"\\\" not supported.\")\n",
    "\n",
    "def _apply_normalization(norm_type, x, *args, **kwargs):\n",
    "    if norm_type == \"batch_norm\":\n",
    "        return batch_norm(x, *args, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Normalization type \\\"\" + norm_type + \"\\\" not supported.\")\n",
    "\n",
    "def _apply_activation(activation_type, x, *args):\n",
    "    if activation_type.lower() == \"relu\":\n",
    "        return tf.nn.relu(x, name=\"Relu\")\n",
    "    elif activation_type.lower() == \"leaky_relu\":\n",
    "        return tf.maximum(x, 0.1 * x, name=\"Leaky_Relu\")\n",
    "    elif activation_type.lower() == \"softmax\":\n",
    "        return tf.nn.softmax(x)\n",
    "    elif activation_type.lower() == \"none\":\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError(\"Activation type \\\"\" + activation_type + \"\\\" not supported.\")\n",
    "        \n",
    "def conv2d(input_layer,\n",
    "           num_outputs,\n",
    "           kernel_size,\n",
    "           stride=1,\n",
    "           padding=\"VALID\",\n",
    "           data_format=\"NCHW\",\n",
    "           normalizer_fn=None,\n",
    "           activation_fn=None,\n",
    "           weights_initializer=\"random_normal\",\n",
    "           biases_initializer=None,\n",
    "           trainable=True,\n",
    "           scope=\"CONV\"):\n",
    "    with tf.name_scope(scope):\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            if data_format == \"NHWC\":\n",
    "                input_channels = input_shape[3]\n",
    "            elif data_format == \"NCHW\":\n",
    "                input_channels = input_shape[1]\n",
    "            W_shape = kernel_size + [input_channels, num_outputs]\n",
    "            if W_init_type == \"xavier\":\n",
    "                layer_shape = input_shape[1:]\n",
    "                n_in = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_in] \n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                                W_shape,\n",
    "                                                *W_init_params)\n",
    "        W = tf.Variable(W_init, \n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "\n",
    "        # Convolute input\n",
    "        stride_h, stride_w = _check_list(stride)\n",
    "        if isinstance(stride_w, list):\n",
    "            if len(stride_w) == 0:\n",
    "                stride_w = stride_h\n",
    "            else:\n",
    "                stride_w = stride_w[0]\n",
    "        if data_format == \"NHWC\":\n",
    "            strides = [1, stride_h, stride_w, 1]\n",
    "        elif data_format == \"NCHW\":\n",
    "            strides = [1, 1, stride_h, stride_w]\n",
    "        out = tf.nn.conv2d(input_layer, \n",
    "                            filter=W,\n",
    "                            strides=strides,\n",
    "                            padding=padding,\n",
    "                            data_format=data_format,\n",
    "                            name=\"convolution\")\n",
    "        \n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=data_format)\n",
    "        \n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            if data_format == \"NHWC\":\n",
    "                b_shape = [1, 1, 1, num_outputs]\n",
    "            elif data_format == \"NCHW\":\n",
    "                b_shape = [1, num_outputs, 1, 1]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "\n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out\n",
    "\n",
    "def conv2d_transpose(x,\n",
    "                     output_shape,\n",
    "                     kernel_size,\n",
    "                     stride=1,\n",
    "                     padding=\"VALID\",\n",
    "                     data_format=\"NCHW\",\n",
    "                     normalizer_fn=None,\n",
    "                     activation_fn=None,\n",
    "                     weights_initializer=\"random_normal\",\n",
    "                     biases_initializer=None,\n",
    "                     trainable=True,\n",
    "                     scope=\"CONV_T\"):\n",
    "    with tf.name_scope(scope):\n",
    "        x_shape = x.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            if data_format == \"NHWC\":\n",
    "                input_channels = x_shape[3]\n",
    "                num_outputs = output_shape[3]\n",
    "            elif data_format == \"NCHW\":\n",
    "                input_channels = x_shape[1]\n",
    "                num_outputs = output_shape[1]\n",
    "            W_shape = kernel_size + [num_outputs, input_channels]\n",
    "            if W_init_type == \"xavier\": # based on output size\n",
    "                layer_shape = output_shape[1:]\n",
    "                n_out = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_out]\n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                               W_shape,\n",
    "                                               *W_init_params)\n",
    "        W = tf.Variable(W_init, \n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "\n",
    "        # Convolute input\n",
    "        stride_h, stride_w = _check_list(stride)\n",
    "        if isinstance(stride_w, list):\n",
    "            if len(stride_w) == 0:\n",
    "                stride_w = stride_h\n",
    "            else:\n",
    "                stride_w = stride_w[0]\n",
    "        if data_format == \"NHWC\":\n",
    "            strides = [1, stride_h, stride_w, 1]\n",
    "        elif data_format == \"NCHW\":\n",
    "            strides = [1, 1, stride_h, stride_w]\n",
    "        out = tf.nn.conv2d_transpose(x, \n",
    "                                     filter=W,\n",
    "                                     output_shape=output_shape,\n",
    "                                     strides=strides,\n",
    "                                     padding=padding,\n",
    "                                     data_format=data_format,\n",
    "                                     name=\"convolution_transpose\")\n",
    "        \n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=data_format)\n",
    "        \n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            if data_format == \"NHWC\":\n",
    "                b_shape = [1, 1, 1, num_outputs]\n",
    "            elif data_format == \"NCHW\":\n",
    "                b_shape = [1, num_outputs, 1, 1]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "\n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def flatten(input_layer, \n",
    "            data_format=\"NCHW\",\n",
    "            scope=\"FLAT\"):\n",
    "    with tf.name_scope(scope):\n",
    "        # Grab runtime values to determine number of elements\n",
    "        input_shape = tf.shape(input_layer)\n",
    "        input_ndims = input_layer.get_shape().ndims\n",
    "        batch_size = tf.slice(input_shape, [0], [1])\n",
    "        layer_shape = tf.slice(input_shape, [1], [input_ndims-1])\n",
    "        num_neurons = tf.expand_dims(tf.reduce_prod(layer_shape), 0)\n",
    "        flattened_shape = tf.concat([batch_size, num_neurons], 0)\n",
    "        if data_format == \"NHWC\":\n",
    "            input_layer = tf.transpose(input_layer, perm=[0, 3, 1, 2])\n",
    "        flat = tf.reshape(input_layer, flattened_shape)\n",
    "        \n",
    "        # Attempt to set values during graph building\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        batch_size, layer_shape = input_shape[0], input_shape[1:]\n",
    "        if all(layer_shape): # None not present\n",
    "            num_neurons = 1\n",
    "            for dim in layer_shape:\n",
    "                num_neurons *= dim\n",
    "            flat.set_shape([batch_size, num_neurons])\n",
    "        else: # None present\n",
    "            flat.set_shape([batch_size, None])\n",
    "        return flat\n",
    "\n",
    "def fully_connected(input_layer,\n",
    "                    num_outputs,\n",
    "                    normalizer_fn=None,\n",
    "                    activation_fn=None,\n",
    "                    weights_initializer=\"random_normal\",\n",
    "                    biases_initializer=None,\n",
    "                    trainable=True,\n",
    "                    scope=\"FC\"):\n",
    "    with tf.name_scope(scope):\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            W_shape = [input_shape[1], num_outputs]\n",
    "            if W_init_type == \"xavier\":\n",
    "                layer_shape = input_shape[1]\n",
    "                n_in = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_in]\n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                            W_shape,\n",
    "                                            *W_init_params)\n",
    "        W = tf.Variable(W_init,\n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "        # Multiply inputs by weights\n",
    "        out = tf.matmul(input_layer, W)\n",
    "\n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=None)\n",
    "\n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            b_shape = [num_outputs]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "       \n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Setttings\n",
    "#data_format = \"NHWC\" # if using cpu\n",
    "data_format = \"NCHW\" # if using gpu\n",
    "results_dir = \"./results/trial_1/\"\n",
    "make_directory(results_dir)\n",
    "time_context = 30\n",
    "feat_size = 512\n",
    "num_sources = 4\n",
    "eps = 1e-18 # numerical stability\n",
    "alpha = 0.001 # learning rate\n",
    "\n",
    "# Data formatting\n",
    "if data_format == \"NHWC\":\n",
    "    input_shape = [None, time_context, feat_size, 1]\n",
    "    target_shape = [None, time_context, feat_size, num_sources]\n",
    "    channel_dim = 3\n",
    "elif data_format == \"NCHW\":\n",
    "    input_shape = [None, 1, time_context, feat_size]\n",
    "    target_shape = [None, num_sources, time_context, feat_size]\n",
    "    channel_dim = 1\n",
    "else:\n",
    "    raise ValueError(\"Unknown data format \\\"\" + data_format + \"\\\"\")\n",
    "spectrogram = tf.placeholder(tf.float32, \n",
    "                             shape=input_shape, \n",
    "                             name=\"magnitude_spectrogram\")\n",
    "\n",
    "# Convolutional layer 1\n",
    "conv1 = conv2d(spectrogram,\n",
    "               num_outputs=30,\n",
    "               kernel_size=[1, 30],\n",
    "               stride=[1, 4],\n",
    "               padding=\"VALID\",\n",
    "               data_format=data_format,\n",
    "               weights_initializer=\"xavier\",\n",
    "               biases_initializer=[\"constant\", 0.0],\n",
    "               scope=\"CONV_1\")\n",
    "\n",
    "# Convolutional layer 2\n",
    "conv2 = conv2d(conv1,\n",
    "               num_outputs=30,\n",
    "               kernel_size=[int(2*time_context/3), 1],\n",
    "               stride=[1, 1],\n",
    "               padding=\"VALID\",\n",
    "               data_format=data_format,\n",
    "               weights_initializer=\"xavier\",\n",
    "               biases_initializer=[\"constant\", 0.0],\n",
    "               scope=\"CONV_2\")\n",
    "conv2_flat = flatten(conv2,\n",
    "                     data_format=data_format,\n",
    "                     scope=\"CONV_2_FLAT\")\n",
    "\n",
    "# Fully-connected layer 1 (encoding)\n",
    "fc1 = fully_connected(conv2_flat,\n",
    "                      num_outputs=256,\n",
    "                      activation_fn=\"relu\",\n",
    "                      weights_initializer=\"xavier\",\n",
    "                      biases_initializer=[\"constant\", 0.0],\n",
    "                      scope=\"FC_1\")\n",
    "\n",
    "# Get shapes for building decoding layers\n",
    "batch_size = tf.shape(spectrogram)[0]\n",
    "conv1_shape = conv1.get_shape().as_list()\n",
    "conv2_shape = conv2.get_shape().as_list()\n",
    "conv2_size = conv2_shape[1] * conv2_shape[2] * conv2_shape[3]\n",
    "\n",
    "# Build decoder for each source\n",
    "fc2, convt1, convt2 = [], [], []\n",
    "for i in range(num_sources):\n",
    "    # Fully-connected layer 2 (decoding)\n",
    "    fc2_i = fully_connected(fc1,\n",
    "                            num_outputs=conv2_size,\n",
    "                            activation_fn=\"relu\",\n",
    "                            weights_initializer=\"xavier\",\n",
    "                            biases_initializer=[\"constant\", 0.0],\n",
    "                            scope=\"FC_2_%d\" % (i+1))\n",
    "    fc2.append(fc2_i)\n",
    "    \n",
    "    # Convolutional transpose layer 1\n",
    "    # Side note: tf.reshape() can infer size of one dimension given rest, so -1 okay\n",
    "    #            tf.nn.conv2d_transpose() must know exact dimensions, but batch size can\n",
    "    #                be inferred at runtime using tf.shape()\n",
    "    fc2_i = tf.reshape(fc2_i, [-1] + conv2_shape[1:])\n",
    "    convt1_i = conv2d_transpose(fc2_i,\n",
    "                                output_shape=[batch_size] + conv1_shape[1:],\n",
    "                                kernel_size=[int(2*time_context/3), 1],\n",
    "                                stride=[1, 1],\n",
    "                                padding=\"VALID\",\n",
    "                                data_format=data_format,\n",
    "                                weights_initializer=\"xavier\",\n",
    "                                biases_initializer=[\"constant\", 0.0],\n",
    "                                scope=\"CONVT_1_%d\" % (i+1))\n",
    "    convt1.append(convt1_i)\n",
    "    \n",
    "    # Convolutional transpose layer 2\n",
    "    convt2_i = conv2d_transpose(convt1_i,\n",
    "                                output_shape=[batch_size] + input_shape[1:],\n",
    "                                kernel_size=[1, 30],\n",
    "                                stride=[1, 4],\n",
    "                                padding=\"VALID\",\n",
    "                                data_format=data_format,\n",
    "                                weights_initializer=\"xavier\",\n",
    "                                biases_initializer=[\"constant\", 0.0],\n",
    "                                scope=\"CONVT_2_%d\" % (i+1))\n",
    "    convt2.append(convt2_i)\n",
    "\n",
    "# Output layer\n",
    "with tf.name_scope(\"y_hat\"):\n",
    "    convt2_all = tf.concat(convt2, axis=channel_dim)\n",
    "    b = tf.Variable(tf.constant(0.0, shape=[1, 1, 1, 1]),\n",
    "                    dtype=tf.float32,\n",
    "                    name=\"bias\")\n",
    "    y_hat = tf.maximum(tf.add(convt2_all, b), 0, name=\"y_hat\")\n",
    "\n",
    "# Masks: m_n(f) = |y_hat_n(f)| / Σ(|y_hat_n'(f)|)\n",
    "with tf.name_scope(\"masks\"):\n",
    "    rand = tf.random_uniform([batch_size] + input_shape[1:])\n",
    "    den = tf.reduce_sum(y_hat, axis=channel_dim, keep_dims=True) + (eps * rand)\n",
    "    masks = tf.div(y_hat, den, name=\"masks\") # broadcast along channel dimension\n",
    "    \n",
    "# Source signals: y_tilde_n(f) = m_n(f) * x(f), \n",
    "# where x(f) is the spectrogram of the input mixture signal\n",
    "with tf.name_scope(\"y_tilde\"):\n",
    "    y_tilde = tf.multiply(masks, spectrogram, name=\"y_tilde\") # broadcast along channel dimension\n",
    "\n",
    "# Loss function: L = Σ(||y_tilde_n - target_n||^2)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    targets = tf.placeholder(tf.float32, \n",
    "                             shape=target_shape, \n",
    "                             name=\"target_sources\")\n",
    "    reduc_indices = [i for i in range(4) if i != channel_dim]\n",
    "    loss_n = tf.reduce_sum(tf.square(y_tilde - targets), axis=reduc_indices, name=\"loss_n\")\n",
    "    loss_total = tf.reduce_sum(loss_n, name=\"loss_total\")\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss_total)\n",
    "\n",
    "# Summaries\n",
    "saver = tf.train.Saver(max_to_keep=5)        \n",
    "graph = tf.get_default_graph()\n",
    "writer = tf.summary.FileWriter(results_dir, graph)\n",
    "loss_sum = []\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    for i in range(num_sources):\n",
    "        loss_sum.append(tf.summary.scalar(\"loss_%d\" % (i+1), loss_n[i]))\n",
    "    loss_sum.append(tf.summary.scalar(\"loss_total\", loss_total))\n",
    "    loss_sum = tf.summary.merge(loss_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3494, 2049)\n"
     ]
    }
   ],
   "source": [
    "def get_shape(shape_file):\n",
    "    \"\"\"Reads a .shape file\"\"\"\n",
    "    with open(shape_file, 'rb') as f:\n",
    "        line=f.readline().decode('ascii')\n",
    "        if line.startswith('#'):\n",
    "            shape=tuple(map(int, re.findall(r'(\\d+)', line)))\n",
    "            return shape\n",
    "        else:\n",
    "            raise IOError('Failed to find shape in file')\n",
    "\n",
    "def create_batches(data, batch_size):\n",
    "    \"\"\"Reshapes data into batches of input size for network\"\"\"\n",
    "    batches = []\n",
    "    time_batches = data.shape[1] // time_context\n",
    "    freq_batches = data.shape[2] // feat_size\n",
    "    for t in range(time_batches):\n",
    "        for f in range(freq_batches):\n",
    "            batches.append(data[:, t*time_context:(t+1)*time_context, f*feat_size:(f+1)*feat_size])\n",
    "    return np.asarray(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "params_dir = results_dir + \"params/\"\n",
    "make_directory(params_dir)\n",
    "input_file = \"./features/02-AchLiebenChristen__m_.data\"\n",
    "shape_file = \"./features/02-AchLiebenChristen__m_.shape\"d\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "f_in = np.fromfile(input_file)\n",
    "if shape_file is not None:\n",
    "    f_shape = get_shape(shape_file)\n",
    "    f_in = np.reshape(f_in, f_shape)\n",
    "input_data = create_batches(f_in[0:1], batch_size) # mixed input\n",
    "target_data = create_batches(f_in[1:], batch_size) # separate sources\n",
    "iter_size = len(input_data) // batch_size\n",
    "\n",
    "# Initialize graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(iter_size):\n",
    "        # Get batch from input data and target data\n",
    "        input_batch = input_data[i*batch_size:(i+1)*batch_size] # magnitude spectrogram of whole\n",
    "        target_batch = target_data[i*batch_size:(i+1)*batch_size] # magnitude spectrogram of sources\n",
    "        \n",
    "        # Perform training step\n",
    "        feed_dict = {spectrogram: input_batch, targets: target_batch}\n",
    "        loss_sum_, _ = sess.run([loss_sum, train_step], \n",
    "                                feed_dict=feed_dict)\n",
    "        writer.add_summary(loss_sum_, global_step=global_step)\n",
    "        writer.flush()\n",
    "        global_step += 1\n",
    "    \n",
    "    # Save model after each epoch\n",
    "    saver.save(sess, params_dir + \"model\", \n",
    "               global_step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(params_file, meta_graph):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348, 1, 30, 513)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_file = None\n",
    "meta_graph = None\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vizdoom]",
   "language": "python",
   "name": "conda-env-vizdoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
