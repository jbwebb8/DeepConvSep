{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Channel Source Separation\n",
    "Some ideas:\n",
    "- Deeper networks\n",
    "- Wider networks\n",
    "- Different conv filter sizes\n",
    "- Noisy inputs\n",
    "- Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _check_list(arg):\n",
    "    if isinstance(arg, list):\n",
    "        try:\n",
    "            return arg[0], arg[1:]\n",
    "        except IndexError:\n",
    "            return arg[0], []\n",
    "    else:\n",
    "        return arg, []\n",
    "\n",
    "def _get_variable_initializer(init_type, var_shape, *args):\n",
    "    if init_type == \"random_normal\":\n",
    "        mean = float(args[0])\n",
    "        stddev = float(args[1])\n",
    "        return tf.random_normal(var_shape, mean=mean, stddev=stddev)\n",
    "    elif init_type == \"truncated_normal\":\n",
    "        mean = float(args[0])\n",
    "        stddev = float(args[1])\n",
    "        return tf.truncated_normal(var_shape, mean=mean, stddev=stddev)\n",
    "    elif init_type == \"constant\":\n",
    "        c = args[0]\n",
    "        return tf.constant(c, dtype=tf.float32, shape=var_shape)\n",
    "    elif init_type == \"xavier\":\n",
    "        n_in = tf.cast(args[0], tf.float32)\n",
    "        return tf.div(tf.random_normal(var_shape), tf.sqrt(n_in))\n",
    "    else:\n",
    "        raise ValueError(\"Variable initializer \\\"\" + init_type + \"\\\" not supported.\")\n",
    "\n",
    "def _apply_normalization(norm_type, x, *args, **kwargs):\n",
    "    if norm_type == \"batch_norm\":\n",
    "        return batch_norm(x, *args, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Normalization type \\\"\" + norm_type + \"\\\" not supported.\")\n",
    "\n",
    "def _apply_activation(activation_type, x, *args):\n",
    "    if activation_type.lower() == \"relu\":\n",
    "        return tf.nn.relu(x, name=\"Relu\")\n",
    "    elif activation_type.lower() == \"leaky_relu\":\n",
    "        return tf.maximum(x, 0.1 * x, name=\"Leaky_Relu\")\n",
    "    elif activation_type.lower() == \"softmax\":\n",
    "        return tf.nn.softmax(x)\n",
    "    elif activation_type.lower() == \"none\":\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError(\"Activation type \\\"\" + activation_type + \"\\\" not supported.\")\n",
    "        \n",
    "def conv2d(input_layer,\n",
    "           num_outputs,\n",
    "           kernel_size,\n",
    "           stride=1,\n",
    "           padding=\"VALID\",\n",
    "           data_format=\"NCHW\",\n",
    "           normalizer_fn=None,\n",
    "           activation_fn=None,\n",
    "           weights_initializer=\"random_normal\",\n",
    "           biases_initializer=None,\n",
    "           trainable=True,\n",
    "           scope=\"CONV\"):\n",
    "    with tf.name_scope(scope):\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            if data_format == \"NHWC\":\n",
    "                input_channels = input_shape[3]\n",
    "            elif data_format == \"NCHW\":\n",
    "                input_channels = input_shape[1]\n",
    "            W_shape = kernel_size + [input_channels, num_outputs]\n",
    "            if W_init_type == \"xavier\":\n",
    "                layer_shape = input_shape[1:]\n",
    "                n_in = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_in] \n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                                W_shape,\n",
    "                                                *W_init_params)\n",
    "        W = tf.Variable(W_init, \n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "\n",
    "        # Convolute input\n",
    "        stride_h, stride_w = _check_list(stride)\n",
    "        if isinstance(stride_w, list):\n",
    "            if len(stride_w) == 0:\n",
    "                stride_w = stride_h\n",
    "            else:\n",
    "                stride_w = stride_w[0]\n",
    "        if data_format == \"NHWC\":\n",
    "            strides = [1, stride_h, stride_w, 1]\n",
    "        elif data_format == \"NCHW\":\n",
    "            strides = [1, 1, stride_h, stride_w]\n",
    "        out = tf.nn.conv2d(input_layer, \n",
    "                            filter=W,\n",
    "                            strides=strides,\n",
    "                            padding=padding,\n",
    "                            data_format=data_format,\n",
    "                            name=\"convolution\")\n",
    "        \n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=data_format)\n",
    "        \n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            if data_format == \"NHWC\":\n",
    "                b_shape = [1, 1, 1, num_outputs]\n",
    "            elif data_format == \"NCHW\":\n",
    "                b_shape = [1, num_outputs, 1, 1]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "\n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out\n",
    "\n",
    "def conv2d_transpose(x,\n",
    "                     output_shape,\n",
    "                     kernel_size,\n",
    "                     stride=1,\n",
    "                     padding=\"VALID\",\n",
    "                     data_format=\"NCHW\",\n",
    "                     normalizer_fn=None,\n",
    "                     activation_fn=None,\n",
    "                     weights_initializer=\"random_normal\",\n",
    "                     biases_initializer=None,\n",
    "                     trainable=True,\n",
    "                     scope=\"CONV_T\"):\n",
    "    with tf.name_scope(scope):\n",
    "        x_shape = x.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            if data_format == \"NHWC\":\n",
    "                input_channels = x_shape[3]\n",
    "                num_outputs = output_shape[3]\n",
    "            elif data_format == \"NCHW\":\n",
    "                input_channels = x_shape[1]\n",
    "                num_outputs = output_shape[1]\n",
    "            W_shape = kernel_size + [num_outputs, input_channels]\n",
    "            if W_init_type == \"xavier\": # based on output size\n",
    "                layer_shape = output_shape[1:]\n",
    "                n_out = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_out]\n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                               W_shape,\n",
    "                                               *W_init_params)\n",
    "        W = tf.Variable(W_init, \n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "\n",
    "        # Convolute input\n",
    "        stride_h, stride_w = _check_list(stride)\n",
    "        if isinstance(stride_w, list):\n",
    "            if len(stride_w) == 0:\n",
    "                stride_w = stride_h\n",
    "            else:\n",
    "                stride_w = stride_w[0]\n",
    "        if data_format == \"NHWC\":\n",
    "            strides = [1, stride_h, stride_w, 1]\n",
    "        elif data_format == \"NCHW\":\n",
    "            strides = [1, 1, stride_h, stride_w]\n",
    "        out = tf.nn.conv2d_transpose(x, \n",
    "                                     filter=W,\n",
    "                                     output_shape=output_shape,\n",
    "                                     strides=strides,\n",
    "                                     padding=padding,\n",
    "                                     data_format=data_format,\n",
    "                                     name=\"convolution_transpose\")\n",
    "        \n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=data_format)\n",
    "        \n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            if data_format == \"NHWC\":\n",
    "                b_shape = [1, 1, 1, num_outputs]\n",
    "            elif data_format == \"NCHW\":\n",
    "                b_shape = [1, num_outputs, 1, 1]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "\n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def flatten(input_layer, \n",
    "            data_format=\"NCHW\",\n",
    "            scope=\"FLAT\"):\n",
    "    with tf.name_scope(scope):\n",
    "        # Grab runtime values to determine number of elements\n",
    "        input_shape = tf.shape(input_layer)\n",
    "        input_ndims = input_layer.get_shape().ndims\n",
    "        batch_size = tf.slice(input_shape, [0], [1])\n",
    "        layer_shape = tf.slice(input_shape, [1], [input_ndims-1])\n",
    "        num_neurons = tf.expand_dims(tf.reduce_prod(layer_shape), 0)\n",
    "        flattened_shape = tf.concat([batch_size, num_neurons], 0)\n",
    "        if data_format == \"NHWC\":\n",
    "            input_layer = tf.transpose(input_layer, perm=[0, 3, 1, 2])\n",
    "        flat = tf.reshape(input_layer, flattened_shape)\n",
    "        \n",
    "        # Attempt to set values during graph building\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        batch_size, layer_shape = input_shape[0], input_shape[1:]\n",
    "        if all(layer_shape): # None not present\n",
    "            num_neurons = 1\n",
    "            for dim in layer_shape:\n",
    "                num_neurons *= dim\n",
    "            flat.set_shape([batch_size, num_neurons])\n",
    "        else: # None present\n",
    "            flat.set_shape([batch_size, None])\n",
    "        return flat\n",
    "\n",
    "def fully_connected(input_layer,\n",
    "                    num_outputs,\n",
    "                    normalizer_fn=None,\n",
    "                    activation_fn=None,\n",
    "                    weights_initializer=\"random_normal\",\n",
    "                    biases_initializer=None,\n",
    "                    trainable=True,\n",
    "                    scope=\"FC\"):\n",
    "    with tf.name_scope(scope):\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            W_shape = [input_shape[1], num_outputs]\n",
    "            if W_init_type == \"xavier\":\n",
    "                layer_shape = input_shape[1]\n",
    "                n_in = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_in]\n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                            W_shape,\n",
    "                                            *W_init_params)\n",
    "        W = tf.Variable(W_init,\n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "        # Multiply inputs by weights\n",
    "        out = tf.matmul(input_layer, W)\n",
    "\n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=None)\n",
    "\n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            b_shape = [num_outputs]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "       \n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "data_format = \"NHWC\" # if using cpu\n",
    "#data_format = \"NCHW\" # if using gpu\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "time_context = 30\n",
    "feat_size = 513\n",
    "num_sources = 4\n",
    "\n",
    "# Input placeholder\n",
    "if data_format == \"NHWC\":\n",
    "    input_shape = [batch_size, time_context, feat_size, 1]\n",
    "elif data_format == \"NCHW\":\n",
    "    input_shape = [batch_size, 1, time_context, feat_size]\n",
    "else:\n",
    "    raise ValueError(\"Unknown data format \\\"\" + data_format + \"\\\"\")\n",
    "spectrogram = tf.placeholder(tf.float32, \n",
    "                             shape=input_shape, \n",
    "                             name=\"magnitude_spectrogram\")\n",
    "\n",
    "# Convolutional layer 1\n",
    "conv1 = conv2d(spectrogram,\n",
    "               num_outputs=30,\n",
    "               kernel_size=[1, 30],\n",
    "               stride=[1, 4],\n",
    "               padding=\"VALID\",\n",
    "               data_format=data_format,\n",
    "               weights_initializer=\"xavier\",\n",
    "               biases_initializer=[\"constant\", 0.0],\n",
    "               scope=\"CONV_1\")\n",
    "\n",
    "# Convolutional layer 2\n",
    "conv2 = conv2d(conv1,\n",
    "               num_outputs=30,\n",
    "               kernel_size=[int(2*time_context/3), 1],\n",
    "               stride=[1, 1],\n",
    "               padding=\"VALID\",\n",
    "               data_format=data_format,\n",
    "               weights_initializer=\"xavier\",\n",
    "               biases_initializer=[\"constant\", 0.0],\n",
    "               scope=\"CONV_2\")\n",
    "conv2_flat = flatten(conv2,\n",
    "                     data_format=data_format,\n",
    "                     scope=\"CONV_2_FLAT\")\n",
    "\n",
    "# Fully-connected layer 1 (encoding)\n",
    "fc1 = fully_connected(conv2_flat,\n",
    "                      num_outputs=256,\n",
    "                      activation_fn=\"relu\",\n",
    "                      weights_initializer=\"xavier\",\n",
    "                      biases_initializer=[\"constant\", 0.0],\n",
    "                      scope=\"FC_1\")\n",
    "\n",
    "spectrogram_shape = spectrogram.get_shape().as_list()\n",
    "conv1_shape = conv1.get_shape().as_list()\n",
    "conv2_shape = conv2.get_shape().as_list()\n",
    "conv2_size = conv2_shape[1] * conv2_shape[2] * conv2_shape[3]\n",
    "fc2, convt1, convt2 = [], [], []\n",
    "for i in range(num_sources):\n",
    "    # Fully-connected layer 2 (decoding)\n",
    "    fc2_i = fully_connected(fc1,\n",
    "                            num_outputs=conv2_size,\n",
    "                            activation_fn=\"relu\",\n",
    "                            weights_initializer=\"xavier\",\n",
    "                            biases_initializer=[\"constant\", 0.0],\n",
    "                            scope=\"FC_2_%d\" % (i+1))\n",
    "    fc2.append(fc2_i)\n",
    "    \n",
    "    # Convolutional transpose layer 1\n",
    "    fc2_i = tf.reshape(fc2_i, conv2_shape)\n",
    "    convt1_i = conv2d_transpose(fc2_i,\n",
    "                                output_shape=conv1_shape,\n",
    "                                kernel_size=[int(2*time_context/3), 1],\n",
    "                                stride=[1, 1],\n",
    "                                padding=\"VALID\",\n",
    "                                data_format=data_format,\n",
    "                                weights_initializer=\"xavier\",\n",
    "                                biases_initializer=[\"constant\", 0.0],\n",
    "                                scope=\"CONVT_1_%d\" % (i+1))\n",
    "    convt1.append(convt1_i)\n",
    "    \n",
    "    # Convolutional transpose layer 2\n",
    "    convt2_i = conv2d_transpose(convt1_i,\n",
    "                                output_shape=spectrogram_shape,\n",
    "                                kernel_size=[1, 30],\n",
    "                                stride=[1, 4],\n",
    "                                padding=\"VALID\",\n",
    "                                data_format=data_format,\n",
    "                                weights_initializer=\"xavier\",\n",
    "                                biases_initializer=[\"constant\", 0.0],\n",
    "                                scope=\"CONVT_2_%d\" % (i+1))\n",
    "    convt2.append(convt2_i)\n",
    "\n",
    "with tf.name_scope(\"masks\"):\n",
    "    masks = tf.concat(convt2, axis=1)\n",
    "    b = tf.Variable(tf.constant(0.0, shape=masks.get_shape().as_list()),\n",
    "                    dtype=tf.float32,\n",
    "                    name=\"bias\")\n",
    "    masks = tf.add(masks, b)\n",
    "    masks = tf.maximum(masks, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vizdoom]",
   "language": "python",
   "name": "conda-env-vizdoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
