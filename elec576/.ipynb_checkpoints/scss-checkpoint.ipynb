{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Channel Source Separation\n",
    "Some ideas:\n",
    "- Deeper networks\n",
    "- Wider networks\n",
    "- Different conv filter sizes\n",
    "- Noisy inputs\n",
    "- Dropout\n",
    "\n",
    "Some notes about data preprocessing:\n",
    "In the original code, data is preprocessed from raw .wav files into numpy arrays containing the magnitude and phase spectrograms from the short time Fourier transfrom (STFT). The class `LargeDataset` in `dataset.py` handles the transformation and subsequent batch handling during network training. How does it work?\n",
    "\n",
    "1. The features (i.e. spectrograms) from the audio files are saved in a directory. If using the `compute_transform` function found in class `Transform` in `transform.py`, then the features for each audio file should be in this directory as:\n",
    "    - {filename}__{m,p}_.data : numpy array containing the magnitude (m) or phase (p) spectrogram\n",
    "    - {filename}__{m,p}_.shape : binary file containing shape of array\n",
    "2. The LargeDataset class is pointed to the feature directory via the `path_transform_in` argument in the constructor.\n",
    "3. It calls updatePath to update its list of .data files (self.file_list), which are all the .data files in the feature directory.\n",
    "4. updatePath also updates the cumulative number of points in the file list (`self.num_points`) and total points (`self.total_points`), where a point is a time window of size `time_context`. This is done via the getNum function, which essentially return the `np.floor(time_axis / time_context`, plus a term if using overlap:\n",
    "\n",
    "```python\n",
    "def getNum(self,id):\n",
    "        \"\"\"\n",
    "        For a single .data file computes the number of examples of size \\\"time_context\\\" that can be created\n",
    "        \"\"\"\n",
    "        shape = self.get_shape(os.path.join(self.path_transform_in[self.dirid[id]],self.file_list[id].replace('.data','.shape')))\n",
    "        time_axis = shape[1]\n",
    "        return np.maximum(1,int(np.floor((time_axis + (np.floor(float(time_axis)/self.time_context) * self.overlap))  / self.time_context)))\n",
    "```\n",
    "5. updatePath also updates the input and output feature sizes via the `getFeatureSize` function, which returns the number of features (self.input_size) and number of features * number of sources (self.output_size) for each .data file.\n",
    "6. Finally, updatePath calls `initBatches()`, which allocates memory needed for output. Several class variables are set:\n",
    "\n",
    "```python\n",
    "self.batch_size = np.minimum(self.batch_size,self.num_points[-1]) # size of each batch\n",
    "self.iteration_size = int(self.total_points / self.batch_size)    # number of batches in dataset\n",
    "self.batch_memory = np.minimum(self.batch_memory,self.iteration_size) # minimum number of batches to load into memory\n",
    "#...\n",
    "self.batch_inputs = np.zeros((self.batch_memory*self.batch_size,self.time_context,self.input_size), dtype=self.tensortype)\n",
    "        self.batch_outputs = np.zeros((self.batch_memory*self.batch_size,self.time_context,self.output_size), dtype=self.tensortype)\n",
    "```\n",
    "7. At this point, all files and directories are accounted for, but nothing has actually been  loaded into memory. `initBatches()` calls `loadBatches()`, which loads batches into `self.batch_inputs` and `self.batch_outputs` once the current store is exhausted, by itself calling `genBatches()`. First, `genBatches()` calls `getNextIndex()` to update class variables that set the time window for the next batch:\n",
    "\n",
    "```python\n",
    "def getNextIndex(self):\n",
    "    \"\"\"\n",
    "    Returns how many batches/sequences to load from each .data file\n",
    "    \"\"\"\n",
    "    # next time point = (# of loads into memory) * (# of time points per load)\n",
    "    target_value = (self.scratch_index+1)*(self.batch_memory*self.batch_size)\n",
    "    # next file index = right-sided search of files with cumulative sum = next time point\n",
    "    idx_target = np.searchsorted(self.num_points,target_value, side='right')\n",
    "    # End case: set idxend to the number of points in the last file, and nindex\n",
    "    # to the last file\n",
    "    if target_value>self.num_points[-1] or idx_target>=len(self.num_points):\n",
    "        idx_target = idx_target - 2\n",
    "        target_value = self.num_points[idx_target]\n",
    "        self.idxend = self.num_points[idx_target] - self.num_points[idx_target-1]\n",
    "        self.nindex = idx_target\n",
    "    # Otherwise, set idxend to number of points after file ending just prior to target\n",
    "    # time point, and nindex to that file\n",
    "    else:\n",
    "        while target_value<=self.num_points[idx_target]:\n",
    "            idx_target = idx_target - 1\n",
    "        self.idxend = target_value - self.num_points[idx_target]\n",
    "        self.nindex = idx_target\n",
    "```\n",
    "8. Next, `genBatches()` decides how much to load from how many files in order to produce batches of the desired length. It utilizes the `loadFile()` function to load .data files into memory between indices `idxbegin` and `idxend`.  As seen in the `loadInputOutput()` helper function, the .data file `file` contains the input, or unseparated STFT, in `allmixinput = file[0]` and the output, or source separated STFTs, in `allmixoutput = file[1:]`. The STFTs are scaled by a (log) scale factor:\n",
    "\n",
    "```python\n",
    "#apply a scaled log10(1+value) function to make sure larger values are eliminated\n",
    "#bach10 training: mult_factor_in = mult_factor_out = 0.3 (0.2 for testing)\n",
    "#                 log_in = log_out = False\n",
    "if self.log_in==True:\n",
    "    allmixinput = self.mult_factor_in*np.log10(1.0+allmixinput)\n",
    "else:\n",
    "    allmixinput = self.mult_factor_in*allmixinput\n",
    "if self.log_out==True:\n",
    "    allmixoutput = self.mult_factor_out*np.log10(1.0+allmixoutput)\n",
    "else:\n",
    "    allmixoutput = self.mult_factor_out*allmixoutput\n",
    "```\n",
    "9. The inputs and outputs in `loadFile()` are originally set via `loadOutput()` to:\n",
    "\n",
    "```python\n",
    "size = idxend - idxbegin\n",
    "inp = np.zeros((size, self.time_context, self.input_size), dtype=self.tensortype)\n",
    "out = np.zeros((size, self.time_context, self.output_size), dtype=self.tensortype)\n",
    "```\n",
    "10. If the file size is smaller than `time_context`, then the first part of `inputs` and `outputs` are taken from the file:\n",
    "\n",
    "```python\n",
    "if self.time_context > allmixinput.shape[1]:\n",
    "    inputs[0,:allmixinput.shape[1],:] = allmixinput[0]\n",
    "    outputs[0, :allmixoutput.shape[1], :allmixoutput.shape[-1]] = allmixoutput[0]\n",
    "    # ...\n",
    "    # concatenate features from rest of sources to third (feature) dimension\n",
    "    for j in range(1,self.nsources):\n",
    "        outputs[0, :allmixoutput.shape[1], j*allmixoutput.shape[-1]:(j+1)*allmixoutput.shape[-1]] = allmixoutput[j]\n",
    "```\n",
    "10. Otherwise, samples of size `time_context` are taken from the file along the time dimension until the target number of loaded samples is satisfied:\n",
    "\n",
    "```python\n",
    "else:\n",
    "    while (start + self.time_context) < allmixinput.shape[1]:\n",
    "        if i>=idxbegin and i<idxend:\n",
    "            # separate variables names for memory clearing\n",
    "            allminput = allmixinput[:,start:start+self.time_context,:] #truncate on time axis so it would match the actual context\n",
    "            allmoutput = allmixoutput[:,start:start+self.time_context,:]\n",
    "            inputs[i-idxbegin] = allminput[0]\n",
    "            outputs[i-idxbegin, :, :allmoutput.shape[-1]] = allmoutput[0]\n",
    "            # ...\n",
    "            # concatenate features from rest of sources to third (feature) dimension\n",
    "            for j in range(1,self.nsources):\n",
    "                outputs[i-idxbegin,:, j*allmoutput.shape[-1]:(j+1)*allmoutput.shape[-1]] = allmoutput[j,:,:]\n",
    "            # ...\n",
    "\n",
    "        i = i + 1\n",
    "        start = start - self.overlap + self.time_context\n",
    "        #clear memory\n",
    "        allminput=None\n",
    "        allmoutput=None\n",
    "```\n",
    "11. `loadFile()` returns a dictionary of input and output (and other) values to `genBatches()`. After smartly loading from the correct number of files in sequence to fill `self.batch_inputs` and `self.batch_outputs`, the batches are shuffled via `shuffleBatches()`. Finally, class variables are incremented accordingly in anticipation of the next call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, errno\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_directory(f):\n",
    "    \"\"\"Makes directory if does not already exist\"\"\"\n",
    "    try:\n",
    "        os.makedirs(f)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _check_list(arg):\n",
    "    if isinstance(arg, list):\n",
    "        try:\n",
    "            return arg[0], arg[1:]\n",
    "        except IndexError:\n",
    "            return arg[0], []\n",
    "    else:\n",
    "        return arg, []\n",
    "\n",
    "def _get_variable_initializer(init_type, var_shape, *args):\n",
    "    if init_type == \"random_normal\":\n",
    "        mean = float(args[0])\n",
    "        stddev = float(args[1])\n",
    "        return tf.random_normal(var_shape, mean=mean, stddev=stddev)\n",
    "    elif init_type == \"truncated_normal\":\n",
    "        mean = float(args[0])\n",
    "        stddev = float(args[1])\n",
    "        return tf.truncated_normal(var_shape, mean=mean, stddev=stddev)\n",
    "    elif init_type == \"constant\":\n",
    "        c = args[0]\n",
    "        return tf.constant(c, dtype=tf.float32, shape=var_shape)\n",
    "    elif init_type == \"xavier\":\n",
    "        n_in = tf.cast(args[0], tf.float32)\n",
    "        return tf.div(tf.random_normal(var_shape), tf.sqrt(n_in))\n",
    "    else:\n",
    "        raise ValueError(\"Variable initializer \\\"\" + init_type + \"\\\" not supported.\")\n",
    "\n",
    "def _apply_normalization(norm_type, x, *args, **kwargs):\n",
    "    if norm_type == \"batch_norm\":\n",
    "        return batch_norm(x, *args, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Normalization type \\\"\" + norm_type + \"\\\" not supported.\")\n",
    "\n",
    "def _apply_activation(activation_type, x, *args):\n",
    "    if activation_type.lower() == \"relu\":\n",
    "        return tf.nn.relu(x, name=\"Relu\")\n",
    "    elif activation_type.lower() == \"leaky_relu\":\n",
    "        return tf.maximum(x, 0.1 * x, name=\"Leaky_Relu\")\n",
    "    elif activation_type.lower() == \"softmax\":\n",
    "        return tf.nn.softmax(x)\n",
    "    elif activation_type.lower() == \"none\":\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError(\"Activation type \\\"\" + activation_type + \"\\\" not supported.\")\n",
    "        \n",
    "def conv2d(input_layer,\n",
    "           num_outputs,\n",
    "           kernel_size,\n",
    "           stride=1,\n",
    "           padding=\"VALID\",\n",
    "           data_format=\"NCHW\",\n",
    "           normalizer_fn=None,\n",
    "           activation_fn=None,\n",
    "           weights_initializer=\"random_normal\",\n",
    "           biases_initializer=None,\n",
    "           trainable=True,\n",
    "           scope=\"CONV\"):\n",
    "    with tf.name_scope(scope):\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            if data_format == \"NHWC\":\n",
    "                input_channels = input_shape[3]\n",
    "            elif data_format == \"NCHW\":\n",
    "                input_channels = input_shape[1]\n",
    "            W_shape = kernel_size + [input_channels, num_outputs]\n",
    "            if W_init_type == \"xavier\":\n",
    "                layer_shape = input_shape[1:]\n",
    "                n_in = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_in] \n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                                W_shape,\n",
    "                                                *W_init_params)\n",
    "        W = tf.Variable(W_init, \n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "\n",
    "        # Convolute input\n",
    "        stride_h, stride_w = _check_list(stride)\n",
    "        if isinstance(stride_w, list):\n",
    "            if len(stride_w) == 0:\n",
    "                stride_w = stride_h\n",
    "            else:\n",
    "                stride_w = stride_w[0]\n",
    "        if data_format == \"NHWC\":\n",
    "            strides = [1, stride_h, stride_w, 1]\n",
    "        elif data_format == \"NCHW\":\n",
    "            strides = [1, 1, stride_h, stride_w]\n",
    "        out = tf.nn.conv2d(input_layer, \n",
    "                            filter=W,\n",
    "                            strides=strides,\n",
    "                            padding=padding,\n",
    "                            data_format=data_format,\n",
    "                            name=\"convolution\")\n",
    "        \n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=data_format)\n",
    "        \n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            if data_format == \"NHWC\":\n",
    "                b_shape = [1, 1, 1, num_outputs]\n",
    "            elif data_format == \"NCHW\":\n",
    "                b_shape = [1, num_outputs, 1, 1]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "\n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out\n",
    "\n",
    "def conv2d_transpose(x,\n",
    "                     output_shape,\n",
    "                     kernel_size,\n",
    "                     stride=1,\n",
    "                     padding=\"VALID\",\n",
    "                     data_format=\"NCHW\",\n",
    "                     normalizer_fn=None,\n",
    "                     activation_fn=None,\n",
    "                     weights_initializer=\"random_normal\",\n",
    "                     biases_initializer=None,\n",
    "                     trainable=True,\n",
    "                     scope=\"CONV_T\"):\n",
    "    with tf.name_scope(scope):\n",
    "        x_shape = x.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            if data_format == \"NHWC\":\n",
    "                input_channels = x_shape[3]\n",
    "                num_outputs = output_shape[3]\n",
    "            elif data_format == \"NCHW\":\n",
    "                input_channels = x_shape[1]\n",
    "                num_outputs = output_shape[1]\n",
    "            W_shape = kernel_size + [num_outputs, input_channels]\n",
    "            if W_init_type == \"xavier\": # based on output size\n",
    "                layer_shape = output_shape[1:]\n",
    "                n_out = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_out]\n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                               W_shape,\n",
    "                                               *W_init_params)\n",
    "        W = tf.Variable(W_init, \n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "\n",
    "        # Convolute input\n",
    "        stride_h, stride_w = _check_list(stride)\n",
    "        if isinstance(stride_w, list):\n",
    "            if len(stride_w) == 0:\n",
    "                stride_w = stride_h\n",
    "            else:\n",
    "                stride_w = stride_w[0]\n",
    "        if data_format == \"NHWC\":\n",
    "            strides = [1, stride_h, stride_w, 1]\n",
    "        elif data_format == \"NCHW\":\n",
    "            strides = [1, 1, stride_h, stride_w]\n",
    "        out = tf.nn.conv2d_transpose(x, \n",
    "                                     filter=W,\n",
    "                                     output_shape=output_shape,\n",
    "                                     strides=strides,\n",
    "                                     padding=padding,\n",
    "                                     data_format=data_format,\n",
    "                                     name=\"convolution_transpose\")\n",
    "        \n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=data_format)\n",
    "        \n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            if data_format == \"NHWC\":\n",
    "                b_shape = [1, 1, 1, num_outputs]\n",
    "            elif data_format == \"NCHW\":\n",
    "                b_shape = [1, num_outputs, 1, 1]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "\n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def flatten(input_layer, \n",
    "            data_format=\"NCHW\",\n",
    "            scope=\"FLAT\"):\n",
    "    with tf.name_scope(scope):\n",
    "        # Grab runtime values to determine number of elements\n",
    "        input_shape = tf.shape(input_layer)\n",
    "        input_ndims = input_layer.get_shape().ndims\n",
    "        batch_size = tf.slice(input_shape, [0], [1])\n",
    "        layer_shape = tf.slice(input_shape, [1], [input_ndims-1])\n",
    "        num_neurons = tf.expand_dims(tf.reduce_prod(layer_shape), 0)\n",
    "        flattened_shape = tf.concat([batch_size, num_neurons], 0)\n",
    "        if data_format == \"NHWC\":\n",
    "            input_layer = tf.transpose(input_layer, perm=[0, 3, 1, 2])\n",
    "        flat = tf.reshape(input_layer, flattened_shape)\n",
    "        \n",
    "        # Attempt to set values during graph building\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        batch_size, layer_shape = input_shape[0], input_shape[1:]\n",
    "        if all(layer_shape): # None not present\n",
    "            num_neurons = 1\n",
    "            for dim in layer_shape:\n",
    "                num_neurons *= dim\n",
    "            flat.set_shape([batch_size, num_neurons])\n",
    "        else: # None present\n",
    "            flat.set_shape([batch_size, None])\n",
    "        return flat\n",
    "\n",
    "def fully_connected(input_layer,\n",
    "                    num_outputs,\n",
    "                    normalizer_fn=None,\n",
    "                    activation_fn=None,\n",
    "                    weights_initializer=\"random_normal\",\n",
    "                    biases_initializer=None,\n",
    "                    trainable=True,\n",
    "                    scope=\"FC\"):\n",
    "    with tf.name_scope(scope):\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        \n",
    "        # Create weights\n",
    "        W_init_type, W_init_params = _check_list(weights_initializer)\n",
    "        with tf.name_scope(W_init_type + \"_initializer\"):\n",
    "            W_shape = [input_shape[1], num_outputs]\n",
    "            if W_init_type == \"xavier\":\n",
    "                layer_shape = input_shape[1]\n",
    "                n_in = tf.reduce_prod(layer_shape)\n",
    "                W_init_params = [n_in]\n",
    "            W_init = _get_variable_initializer(W_init_type,\n",
    "                                            W_shape,\n",
    "                                            *W_init_params)\n",
    "        W = tf.Variable(W_init,\n",
    "                        dtype=tf.float32, \n",
    "                        trainable=trainable, \n",
    "                        name=\"weights\")\n",
    "        \n",
    "        # Multiply inputs by weights\n",
    "        out = tf.matmul(input_layer, W)\n",
    "\n",
    "        # Apply normalization\n",
    "        if normalizer_fn is not None:\n",
    "            norm_type, norm_params = _check_list(normalizer_fn)\n",
    "            out = _apply_normalization(norm_type, \n",
    "                                       out, \n",
    "                                       *norm_params,\n",
    "                                       data_format=None)\n",
    "\n",
    "        # Add biases\n",
    "        elif biases_initializer is not None:\n",
    "            b_init_type, b_init_params = _check_list(biases_initializer)\n",
    "            b_shape = [num_outputs]\n",
    "            b_init = _get_variable_initializer(b_init_type,\n",
    "                                               b_shape,\n",
    "                                               *b_init_params)\n",
    "            b = tf.Variable(b_init,\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=trainable,\n",
    "                            name=\"biases\")\n",
    "            out = tf.add(out, b, name=\"BiasAdd\")\n",
    "       \n",
    "        # Apply activation\n",
    "        if activation_fn is not None:\n",
    "            act_type, act_params = _check_list(activation_fn)\n",
    "            out = _apply_activation(act_type, out, *act_params)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Setttings\n",
    "#data_format = \"NHWC\" # if using cpu\n",
    "data_format = \"NCHW\" # if using gpu\n",
    "results_dir = \"./results/trial_1/\"\n",
    "make_directory(results_dir)\n",
    "time_context = 30\n",
    "feat_size = 512\n",
    "num_sources = 4\n",
    "eps = 1e-18 # numerical stability\n",
    "alpha = 0.001 # learning rate\n",
    "\n",
    "# Data formatting\n",
    "if data_format == \"NHWC\":\n",
    "    input_shape = [None, time_context, feat_size, 1]\n",
    "    target_shape = [None, time_context, feat_size, num_sources]\n",
    "    channel_dim = 3\n",
    "elif data_format == \"NCHW\":\n",
    "    input_shape = [None, 1, time_context, feat_size]\n",
    "    target_shape = [None, num_sources, time_context, feat_size]\n",
    "    channel_dim = 1\n",
    "else:\n",
    "    raise ValueError(\"Unknown data format \\\"\" + data_format + \"\\\"\")\n",
    "spectrogram = tf.placeholder(tf.float32, \n",
    "                             shape=input_shape, \n",
    "                             name=\"magnitude_spectrogram\")\n",
    "\n",
    "# Convolutional layer 1\n",
    "conv1 = conv2d(spectrogram,\n",
    "               num_outputs=30,\n",
    "               kernel_size=[1, 30],\n",
    "               stride=[1, 4],\n",
    "               padding=\"VALID\",\n",
    "               data_format=data_format,\n",
    "               weights_initializer=\"xavier\",\n",
    "               biases_initializer=[\"constant\", 0.0],\n",
    "               scope=\"CONV_1\")\n",
    "\n",
    "# Convolutional layer 2\n",
    "conv2 = conv2d(conv1,\n",
    "               num_outputs=30,\n",
    "               kernel_size=[int(2*time_context/3), 1],\n",
    "               stride=[1, 1],\n",
    "               padding=\"VALID\",\n",
    "               data_format=data_format,\n",
    "               weights_initializer=\"xavier\",\n",
    "               biases_initializer=[\"constant\", 0.0],\n",
    "               scope=\"CONV_2\")\n",
    "conv2_flat = flatten(conv2,\n",
    "                     data_format=data_format,\n",
    "                     scope=\"CONV_2_FLAT\")\n",
    "\n",
    "# Fully-connected layer 1 (encoding)\n",
    "fc1 = fully_connected(conv2_flat,\n",
    "                      num_outputs=256,\n",
    "                      activation_fn=\"relu\",\n",
    "                      weights_initializer=\"xavier\",\n",
    "                      biases_initializer=[\"constant\", 0.0],\n",
    "                      scope=\"FC_1\")\n",
    "\n",
    "# Get shapes for building decoding layers\n",
    "batch_size = tf.shape(spectrogram)[0]\n",
    "conv1_shape = conv1.get_shape().as_list()\n",
    "conv2_shape = conv2.get_shape().as_list()\n",
    "conv2_size = conv2_shape[1] * conv2_shape[2] * conv2_shape[3]\n",
    "\n",
    "# Build decoder for each source\n",
    "fc2, convt1, convt2 = [], [], []\n",
    "for i in range(num_sources):\n",
    "    # Fully-connected layer 2 (decoding)\n",
    "    fc2_i = fully_connected(fc1,\n",
    "                            num_outputs=conv2_size,\n",
    "                            activation_fn=\"relu\",\n",
    "                            weights_initializer=\"xavier\",\n",
    "                            biases_initializer=[\"constant\", 0.0],\n",
    "                            scope=\"FC_2_%d\" % (i+1))\n",
    "    fc2.append(fc2_i)\n",
    "    \n",
    "    # Convolutional transpose layer 1\n",
    "    # Side note: tf.reshape() can infer size of one dimension given rest, so -1 okay\n",
    "    #            tf.nn.conv2d_transpose() must know exact dimensions, but batch size can\n",
    "    #                be inferred at runtime using tf.shape()\n",
    "    fc2_i = tf.reshape(fc2_i, [-1] + conv2_shape[1:])\n",
    "    convt1_i = conv2d_transpose(fc2_i,\n",
    "                                output_shape=[batch_size] + conv1_shape[1:],\n",
    "                                kernel_size=[int(2*time_context/3), 1],\n",
    "                                stride=[1, 1],\n",
    "                                padding=\"VALID\",\n",
    "                                data_format=data_format,\n",
    "                                weights_initializer=\"xavier\",\n",
    "                                biases_initializer=[\"constant\", 0.0],\n",
    "                                scope=\"CONVT_1_%d\" % (i+1))\n",
    "    convt1.append(convt1_i)\n",
    "    \n",
    "    # Convolutional transpose layer 2\n",
    "    convt2_i = conv2d_transpose(convt1_i,\n",
    "                                output_shape=[batch_size] + input_shape[1:],\n",
    "                                kernel_size=[1, 30],\n",
    "                                stride=[1, 4],\n",
    "                                padding=\"VALID\",\n",
    "                                data_format=data_format,\n",
    "                                weights_initializer=\"xavier\",\n",
    "                                biases_initializer=[\"constant\", 0.0],\n",
    "                                scope=\"CONVT_2_%d\" % (i+1))\n",
    "    convt2.append(convt2_i)\n",
    "\n",
    "# Output layer\n",
    "with tf.name_scope(\"y_hat\"):\n",
    "    convt2_all = tf.concat(convt2, axis=channel_dim)\n",
    "    b = tf.Variable(tf.constant(0.0, shape=[1, 1, 1, 1]),\n",
    "                    dtype=tf.float32,\n",
    "                    name=\"bias\")\n",
    "    y_hat = tf.maximum(tf.add(convt2_all, b), 0, name=\"y_hat\")\n",
    "\n",
    "# Masks: m_n(f) = |y_hat_n(f)| / Σ(|y_hat_n'(f)|)\n",
    "with tf.name_scope(\"masks\"):\n",
    "    rand = tf.random_uniform([batch_size] + input_shape[1:])\n",
    "    den = tf.reduce_sum(y_hat, axis=channel_dim, keep_dims=True) + (eps * rand)\n",
    "    masks = tf.div(y_hat, den, name=\"masks\") # broadcast along channel dimension\n",
    "    \n",
    "# Source signals: y_tilde_n(f) = m_n(f) * x(f), \n",
    "# where x(f) is the spectrogram of the input mixture signal\n",
    "with tf.name_scope(\"y_tilde\"):\n",
    "    y_tilde = tf.multiply(masks, spectrogram, name=\"y_tilde\") # broadcast along channel dimension\n",
    "\n",
    "# Loss function: L = Σ(||y_tilde_n - target_n||^2)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    targets = tf.placeholder(tf.float32, \n",
    "                             shape=target_shape, \n",
    "                             name=\"target_sources\")\n",
    "    reduc_indices = [i for i in range(4) if i != channel_dim]\n",
    "    loss_n = tf.reduce_sum(tf.square(y_tilde - targets), axis=reduc_indices, name=\"loss_n\")\n",
    "    loss_total = tf.reduce_sum(loss_n, name=\"loss_total\")\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss_total)\n",
    "\n",
    "# Summaries\n",
    "saver = tf.train.Saver(max_to_keep=5)        \n",
    "graph = tf.get_default_graph()\n",
    "writer = tf.summary.FileWriter(results_dir, graph)\n",
    "loss_sum = []\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    for i in range(num_sources):\n",
    "        loss_sum.append(tf.summary.scalar(\"loss_%d\" % (i+1), loss_n[i]))\n",
    "    loss_sum.append(tf.summary.scalar(\"loss_total\", loss_total))\n",
    "    loss_sum = tf.summary.merge(loss_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3494, 2049)\n"
     ]
    }
   ],
   "source": [
    "def get_shape(shape_file):\n",
    "    \"\"\"Reads a .shape file\"\"\"\n",
    "    with open(shape_file, 'rb') as f:\n",
    "        line=f.readline().decode('ascii')\n",
    "        if line.startswith('#'):\n",
    "            shape=tuple(map(int, re.findall(r'(\\d+)', line)))\n",
    "            return shape\n",
    "        else:\n",
    "            raise IOError('Failed to find shape in file')\n",
    "\n",
    "def create_batches(data, batch_size):\n",
    "    \"\"\"Reshapes data into batches of input size for network\"\"\"\n",
    "    batches = []\n",
    "    time_batches = data.shape[1] // time_context\n",
    "    freq_batches = data.shape[2] // feat_size\n",
    "    for t in range(time_batches):\n",
    "        for f in range(freq_batches):\n",
    "            batches.append(data[:, t*time_context:(t+1)*time_context, f*feat_size:(f+1)*feat_size])\n",
    "    return np.asarray(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "params_dir = results_dir + \"params/\"\n",
    "make_directory(params_dir)\n",
    "input_file = \"./features/02-AchLiebenChristen__m_.data\"\n",
    "shape_file = \"./features/02-AchLiebenChristen__m_.shape\"d\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "f_in = np.fromfile(input_file)\n",
    "if shape_file is not None:\n",
    "    f_shape = get_shape(shape_file)\n",
    "    f_in = np.reshape(f_in, f_shape)\n",
    "input_data = create_batches(f_in[0:1], batch_size) # mixed input\n",
    "target_data = create_batches(f_in[1:], batch_size) # separate sources\n",
    "iter_size = len(input_data) // batch_size\n",
    "\n",
    "# Initialize graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(iter_size):\n",
    "        # Get batch from input data and target data\n",
    "        input_batch = input_data[i*batch_size:(i+1)*batch_size] # magnitude spectrogram of whole\n",
    "        target_batch = target_data[i*batch_size:(i+1)*batch_size] # magnitude spectrogram of sources\n",
    "        \n",
    "        # Perform training step\n",
    "        feed_dict = {spectrogram: input_batch, targets: target_batch}\n",
    "        loss_sum_, _ = sess.run([loss_sum, train_step], \n",
    "                                feed_dict=feed_dict)\n",
    "        writer.add_summary(loss_sum_, global_step=global_step)\n",
    "        writer.flush()\n",
    "        global_step += 1\n",
    "    \n",
    "    # Save model after each epoch\n",
    "    saver.save(sess, params_dir + \"model\", \n",
    "               global_step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(params_file, meta_graph):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348, 1, 30, 513)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_file = None\n",
    "meta_graph = None\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
